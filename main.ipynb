{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, precision_score, matthews_corrcoef\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from ticktock import tick, tock\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, either by generating it from the raw .txt files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.panelizeRawData('data/price_raw_hr.p', 'data/vol_raw_hr.p', 'data/panel.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or loading it from a pickled file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/panel.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should now have the form:\n",
    "```<class 'pandas.core.panel.Panel'>\n",
    "Dimensions: 2 (items) x 95208 (major_axis) x 4 (minor_axis)\n",
    "Items axis: price to vol\n",
    "Major_axis axis: 2002-01-02 19:00:00-05:00 to 2017-05-18 18:00:00-04:00\n",
    "Minor_axis axis: 0 to 3\n",
    "```\n",
    "Now the slow part: computing the technical indicators for each desired timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.computeData(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or load the computed data from a file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/dataframe.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(df):\n",
    "\ts = df['s']\n",
    "\tw = df['w']\n",
    "\treturn pd.Series({\n",
    "\t\t'z': proportions_ztest(sum(s & w).sum(), sum(s), value=0.5, alternative='two-sided')[0] if sum(s) else np.NaN,\n",
    "\t\t'confusion_matrix': confusion_matrix(w, s),\n",
    "\t\t'f1': f1_score(w, s),\n",
    "\t\t'accuracy': accuracy_score(w, s),\n",
    "\t\t'precision': precision_score(w, s),\n",
    "\t\t'matthews': matthews_corrcoef(w, s)\n",
    "\t})\n",
    "\n",
    "tick()\n",
    "summ = data.groupby(['mkt', 'lkbk', 'wkdy', 'wk#', 'time']).apply(summarize)\n",
    "summ['abs z'] = np.abs(summ['z'])\n",
    "summ['NOBS'] = summ['confusion_matrix'].map(lambda i: sum(i.flatten()))\n",
    "tock('summarize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d2[\n",
    "\t(d2.mkt == 0) &\n",
    "\t(d2.lkbk == 5) &\n",
    "\t(d2.wkdy == 4) &\n",
    "\t(d2['wk#'] == 1) &\n",
    "\t(d2.time == 14)\n",
    "\t]\n",
    "\n",
    "catcols = [\n",
    "\t# 'mkt',\n",
    "\t# 'time',\n",
    "\t# 'lkbk',\n",
    "\t# 'wkdy',\n",
    "\t# 'wk#',\n",
    "]\n",
    "\n",
    "numcols = [\n",
    "\t# 'macd',\n",
    "\t# 'obv',\n",
    "\t# 'p_2ma',\n",
    "\t# 'p_avg',\n",
    "\t# 'p_krt',\n",
    "\t# 'p_med',\n",
    "\t# 'p_skw',\n",
    "\t# 'p_std',\n",
    "\t# 'rsi',\n",
    "\t# 'v_avg',\n",
    "\t# 'v_cur',\n",
    "\t# 'v_krt',\n",
    "\t# 'v_med',\n",
    "\t# 'v_skw',\n",
    "\t# 'v_std',\n",
    "\t'prd_rtn',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is a time series, cross-validation is a little easier: the test set will always be the most recent data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = dt.datetime(2017, 1, 1)\n",
    "\n",
    "man_results = summarize(x[x.index >= split])\n",
    "\n",
    "x1 = x.loc[x.index < split, catcols + numcols]\n",
    "y1 = x.loc[x.index < split, 'fwd_rtn']\n",
    "x2 = x.loc[x.index >= split, catcols + numcols]\n",
    "y2 = x.loc[x.index >= split, 'fwd_rtn']\n",
    "\n",
    "tick()\n",
    "\n",
    "ss = StandardScaler().fit(x1[numcols])\n",
    "if catcols:\n",
    "\toe = OneHotEncoder().fit(x1[catcols])\n",
    "\tx1 = np.concatenate((oe.transform(x1[catcols]).toarray(), ss.transform(x1[numcols])), axis=1)\n",
    "\tx2 = np.concatenate((oe.transform(x2[catcols]).toarray(), ss.transform(x2[numcols])), axis=1)\n",
    "else:\n",
    "\tx1 = ss.transform(x1[numcols])\n",
    "\tx2 = ss.transform(x2[numcols])\n",
    "  # pca = PCA(n_components=3).fit(x1)\n",
    "  # x1 = pca.transform(x1)\n",
    "  # x2 = pca.transform(x2)\n",
    "\n",
    "clfs = {\n",
    "\t'LSVC': [LinearSVC(), {\n",
    "\t\t'C': [1, 10, 100],\n",
    "\t}],\n",
    "\t'SVC': [SVC(), {\n",
    "\t\t'C': [1, 10, 100],\n",
    "\t\t'kernel': ['linear', 'rbf'],\n",
    "\t\t'gamma': [0, 0.1, 1, 10],\n",
    "\t\t'probability': [True, False]\n",
    "\t}],\n",
    "\t'SGD': [SGDClassifier(), {\n",
    "\t\t'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "\t\t'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "\t\t'alpha': [0.0001, 0.001, 0.01],\n",
    "\t}],\n",
    "\t'RFC': [RandomForestClassifier(), {\n",
    "\t\t'n_estimators': [10, 50, 100],\n",
    "\t\t'criterion': ['gini', 'entropy'],\n",
    "\t\t'max_features': [None, 'auto'],\n",
    "\t\t'min_samples_split': [10, 50, 100],\n",
    "\t}],\n",
    "\t'KNN': [KNeighborsClassifier(), {\n",
    "\t\t'n_neighbors': [5, 10, 30],\n",
    "\t\t'weights': ['uniform', 'distance'],\n",
    "\t\t'algorithm': ['ball_tree', 'kd_tree', 'brute', 'auto'],\n",
    "\t\t'p': [1, 2, 3],\n",
    "\t}],\n",
    "\t'GBC': [GradientBoostingClassifier(), {\n",
    "\t\t'loss': ['deviance', 'exponential', 'deviance'],\n",
    "\t\t'learning_rate': [0.01, 0.1, 1],\n",
    "\t\t'max_depth': [2, 3, 4],\n",
    "\t}],\n",
    "\t'MLP': [MLPClassifier(), {\n",
    "\t\t'hidden_layer_sizes': [(5, 2)],\n",
    "\t\t'activation': ['logistic', 'relu'],\n",
    "\t\t'solver': ['lbfgs'],\n",
    "\t\t'alpha': [1e-5],\n",
    "\t\t'learning_rate': ['constant', 'adaptive'],\n",
    "\t}]\n",
    "}\n",
    "\n",
    "def classify(params, x1, y1, x2, y2):\n",
    "\timport time\n",
    "\tnow = time.clock()\n",
    "\tgscv = GridSearchCV(*params, scoring='f1').fit(x1, y1)\n",
    "\tt = time.clock() - now\n",
    "\tpred2 = gscv.predict(x2)\n",
    "\treturn {'metrics': {\n",
    "\t\t'precision': precision_score(y2, pred2),\n",
    "\t\t'accuracy': accuracy_score(y2, pred2),\n",
    "\t\t'f1': f1_score(y2, pred2),\n",
    "\t\t'matthews': matthews_corrcoef(y2, pred2),\n",
    "\t\t'training accuracy': accuracy_score(y1, gscv.best_estimator_.predict(x1)),\n",
    "\t},\n",
    "\t\t'best_classifier': gscv.best_estimator_,\n",
    "\t\t'other': {\n",
    "\t\t\t'confusion_matrix': confusion_matrix(y2, pred2),\n",
    "\t\t\t'best_params': gscv.best_params_,\n",
    "\t\t\t'training time': t,\n",
    "\t\t},\n",
    "\t}\n",
    "\t\n",
    "results = {name: classify(params, x1=x1, y1=y1, x2=x2, y2=y2) for (name, params) in clfs.items()}\n",
    "metrics = pd.DataFrame([p['metrics'] for (n, p) in results.items()], index=results.keys())\n",
    "metrics = metrics.append(pd.Series(man_results[['f1', 'accuracy', 'matthews', 'precision']], name='Manual'))\n",
    "metrics.index.set_names('Classifier', inplace=True)\n",
    "metrics.reset_index(inplace=True)\n",
    "metrics = pd.melt(metrics, id_vars='Classifier', var_name='Metric')\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(results) + 1)\n",
    "for n, p in results.items():\n",
    "\ti = list(results.keys()).index(n)\n",
    "\t# plt.xlabel('pred')\n",
    "\t# plt.ylabel('true')\n",
    "\t# lab = ['N', 'P'] if i == 0 else False\n",
    "\tsns.heatmap(p['other']['confusion_matrix'], annot=True, cbar=False, square=True, ax=ax[i])\n",
    "\tax[i].set_title(n)\n",
    "sns.heatmap(man_results['confusion_matrix'], annot=True, cbar=False, square=True, ax=ax[len(results)])\n",
    "ax[len(results)].set_title('Manual')\n",
    "plt.figure()\n",
    "sns.barplot(data=metrics, hue='Classifier', x='Metric', y='value');\n",
    "for n, p in results.items():\n",
    "\tprint(n)\n",
    "\tprint(pd.Series(p['other']['best_params']))\n",
    "\tprint('----------------------------')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
